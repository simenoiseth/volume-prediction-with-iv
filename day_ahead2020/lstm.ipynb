{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0247184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LSTM for Next-Day Trading Volume (from your CSVs) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load & merge data (keep your split)\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"train_volume_vix.csv\", parse_dates=[\"date\"])\n",
    "test  = pd.read_csv(\"test_volume_vix.csv\",  parse_dates=[\"date\"])\n",
    "\n",
    "train = train.sort_values(\"date\").reset_index(drop=True)\n",
    "test  = test.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "split_date = test[\"date\"].min()\n",
    "df = pd.concat([train, test], ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Feature engineering (no leakage)\n",
    "#    We build features that are known up to day t to predict target at t+1\n",
    "# -----------------------------\n",
    "# Basic transforms\n",
    "df[\"log_volume_t\"] = np.log(df[\"total_dollar_volume\"])\n",
    "\n",
    "# VIX dynamics\n",
    "df[\"vix_lag1\"] = df[\"vix_close\"].shift(1)\n",
    "df[\"vix_change\"] = df[\"vix_close\"] - df[\"vix_lag1\"]\n",
    "df[\"vix_5d_ma\"] = df[\"vix_close\"].rolling(window=5).mean()\n",
    "\n",
    "# Target: predict next-day volume (already provided as target_volume)\n",
    "# We'll model y_log = log(target_volume)\n",
    "df[\"y_log\"] = np.log(df[\"target_volume\"])\n",
    "\n",
    "# VIX\n",
    "# -----------------------------\n",
    "# 3) Select feature columns\n",
    "#    (sequence model learns temporal patterns; no manual lags needed)\n",
    "# -----------------------------\n",
    "feature_cols = [\"log_volume_t\"]\n",
    "\n",
    "df = df.dropna(subset=feature_cols).reset_index(drop=True)\n",
    "\n",
    "feat_df = df[[\"date\"] + feature_cols + [\"y_log\"]].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Split back to train/test by date\n",
    "#    (windows for earliest test targets will include history from train, which is OK)\n",
    "# -----------------------------\n",
    "\n",
    "start_date = pd.Timestamp(\"2020-03-01\")\n",
    "df = df[df[\"date\"] >= start_date].reset_index(drop=True)\n",
    "\n",
    "train_df = feat_df[feat_df[\"date\"] < split_date].copy()\n",
    "test_df  = feat_df[feat_df[\"date\"] >= split_date].copy()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Scale features and target using train only\n",
    "# -----------------------------\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_train_2d = X_scaler.fit_transform(train_df[feature_cols].values)\n",
    "X_test_2d  = X_scaler.transform(test_df[feature_cols].values)\n",
    "\n",
    "y_train_1d = y_scaler.fit_transform(train_df[[\"y_log\"]].values).ravel()\n",
    "y_test_1d  = y_scaler.transform(test_df[[\"y_log\"]].values).ravel()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Build sequences (rolling windows)\n",
    "#    LOOKBACK = number of past days the LSTM sees to predict t+1\n",
    "# -----------------------------\n",
    "LOOKBACK = 30  # try 30 trading days; adjust as you like\n",
    "\n",
    "def make_sequences(X_2d, y_1d, dates, lookback):\n",
    "    X_list, y_list, d_list = [], [], []\n",
    "    for i in range(lookback, len(X_2d)):\n",
    "        X_list.append(X_2d[i-lookback:i, :])   # window [i-lookback, ..., i-1]\n",
    "        y_list.append(y_1d[i])                 # target aligned at i (predict next-day log vol)\n",
    "        d_list.append(dates.iloc[i])\n",
    "    return np.array(X_list), np.array(y_list), pd.Series(d_list)\n",
    "\n",
    "# Build sequences on the FULL timeline so test windows can include train history,\n",
    "# then split sequences by their TARGET DATE (d_list) relative to split_date.\n",
    "X_all_2d = X_scaler.transform(feat_df[feature_cols].values)  # transform entire series\n",
    "y_all_1d = y_scaler.transform(feat_df[[\"y_log\"]].values).ravel()\n",
    "dates_all = feat_df[\"date\"].reset_index(drop=True)\n",
    "\n",
    "X_seq, y_seq, d_seq = make_sequences(X_all_2d, y_all_1d, dates_all, LOOKBACK)\n",
    "\n",
    "train_mask = d_seq < split_date\n",
    "test_mask  = d_seq >= split_date\n",
    "\n",
    "X_train, y_train_seq = X_seq[train_mask], y_seq[train_mask]\n",
    "X_test,  y_test_seq  = X_seq[test_mask],  y_seq[test_mask]\n",
    "dates_test_seq = d_seq[test_mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Train sequences:\", X_train.shape, \" | Test sequences:\", X_test.shape)\n",
    "\n",
    "print(len(X_train))\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Define LSTM model\n",
    "# -----------------------------\n",
    "n_features = X_train.shape[-1]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(LOOKBACK, n_features)),\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=\"mse\",\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Train/validation split (time-aware: last slice of train as val)\n",
    "# -----------------------------\n",
    "val_frac = 0.15\n",
    "val_size = int(len(X_train) * val_frac)\n",
    "X_tr, X_val = X_train[:-val_size], X_train[-val_size:]\n",
    "y_tr, y_val = y_train_seq[:-val_size], y_train_seq[-val_size:]\n",
    "\n",
    "hist = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[es, rlr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Inference & metrics\n",
    "#    (invert scaling to log-space, then to original volume)\n",
    "# -----------------------------\n",
    "# Predict (scaled y)\n",
    "y_pred_scaled = model.predict(X_test).ravel()\n",
    "\n",
    "# Back to log scale\n",
    "y_pred_log = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "y_test_log = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Back to original scale\n",
    "pred = np.exp(y_pred_log)\n",
    "actual = np.exp(y_test_log)\n",
    "\n",
    "# Metrics\n",
    "mse_log = mean_squared_error(y_test_log, y_pred_log)\n",
    "rmse_log = np.sqrt(mse_log)\n",
    "mae_log = mean_absolute_error(y_test_log, y_pred_log)\n",
    "r2_log = r2_score(y_test_log, y_pred_log)\n",
    "\n",
    "mse_orig = mean_squared_error(actual, pred)\n",
    "rmse_orig = np.sqrt(mse_orig)\n",
    "mae_orig = mean_absolute_error(actual, pred)\n",
    "mape = (np.abs(pred - actual) / actual).mean() * 100\n",
    "\n",
    "print(\"\\n=== LSTM Performance ===\")\n",
    "print(f\"Log scale -> RMSE: {rmse_log:.4f}  MAE: {mae_log:.4f}  RÂ²: {r2_log:.3f}\")\n",
    "print(f\"Original  -> RMSE: {rmse_orig:,.0f}  MAE: {mae_orig:,.0f}  MAPE: {mape:.2f}%\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 10) Plots\n",
    "# -----------------------------\n",
    "# Loss curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE (scaled)\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Actual vs Predicted (log scale)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_test_log, y_pred_log, alpha=0.6)\n",
    "lo, hi = min(y_test_log.min(), y_pred_log.min()), max(y_test_log.max(), y_pred_log.max())\n",
    "plt.plot([lo, hi], [lo, hi], 'r--', lw=2)\n",
    "plt.xlabel(\"Actual log(Next-Day Volume)\")\n",
    "plt.ylabel(\"Predicted log(Next-Day Volume)\")\n",
    "plt.title(\"LSTM: Actual vs Predicted (log scale)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Time series (original scale)\n",
    "plot_df = pd.DataFrame({\n",
    "    \"date\": dates_test_seq,\n",
    "    \"actual_volume\": actual,\n",
    "    \"predicted_volume\": pred\n",
    "})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(plot_df[\"date\"], plot_df[\"actual_volume\"], label=\"Actual\", alpha=0.85)\n",
    "plt.plot(plot_df[\"date\"], plot_df[\"predicted_volume\"], label=\"Predicted\", alpha=0.85)\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Trading Volume\")\n",
    "plt.title(\"LSTM: Actual vs Predicted (original scale)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
