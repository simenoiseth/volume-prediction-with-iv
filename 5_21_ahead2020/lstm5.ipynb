{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19241e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# LSTM for 5-day accumulated volume using only lagged volume sequences\n",
    "# =====================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 0) CONFIG\n",
    "# -----------------------------\n",
    "WINDOW = 30          # past trading days fed to the LSTM\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "PATIENCE = 10        # early stopping\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1) LOAD & MERGE\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"5_21_train.csv\", parse_dates=[\"date\"])\n",
    "test  = pd.read_csv(\"5_21_test.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "train = train.sort_values(\"date\").reset_index(drop=True)\n",
    "test  = test.sort_values(\"date\").reset_index(drop=True)\n",
    "split_date = test[\"date\"].min()\n",
    "\n",
    "# Combine to ensure sequences that straddle the split are handled correctly\n",
    "df = pd.concat([train, test], ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f698562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 2) KEEP ONLY WHAT WE NEED (no VIX yet)\n",
    "# -----------------------------\n",
    "# We’ll only use log(sh_volume) as the single feature for the LSTM.\n",
    "df[\"log_vol\"] = np.log(df[\"sh_volume\"])\n",
    "\n",
    "# Target is already prepared in your CSV as the accumulated next 5 trading days.\n",
    "# We predict on log scale (consistent with your RF code).\n",
    "df[\"y_log\"] = np.log(df[\"target_5d\"])\n",
    "\n",
    "# Drop any rows that still have NaNs (e.g., the very beginning or end)\n",
    "df = df.dropna(subset=[\"log_vol\", \"y_log\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380cfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 3) TRAIN / TEST SPLIT BY DATE (no leakage)\n",
    "# -----------------------------\n",
    "start_date = pd.Timestamp(\"2020-01-01\")\n",
    "df = df[df[\"date\"] >= start_date].reset_index(drop=True)\n",
    "\n",
    "# Re-split using the stored split_date\n",
    "train_df = df[df[\"date\"] < split_date].copy()\n",
    "test_df  = df[df[\"date\"] >= split_date].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eabf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 4) SCALE FEATURES USING TRAIN ONLY\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[[\"log_vol\"]])\n",
    "\n",
    "train_df[\"log_vol_scaled\"] = scaler.transform(train_df[[\"log_vol\"]])\n",
    "test_df[\"log_vol_scaled\"]  = scaler.transform(test_df[[\"log_vol\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5) BUILD SEQUENCES (WINDOW past days -> predict y at day t)\n",
    "#    For each index t, X contains [log_vol_scaled at t-WINDOW+1 ... t]\n",
    "#    and y contains y_log at t (target_5d is sum of t+1..t+5 -> safe)\n",
    "# -----------------------------\n",
    "def make_sequences(frame, window, feature_col=\"log_vol_scaled\", target_col=\"y_log\"):\n",
    "    X_list, y_list, idx_list = [], [], []\n",
    "    values = frame[feature_col].values.astype(np.float32)\n",
    "    targets = frame[target_col].values.astype(np.float32)\n",
    "    for t in range(window - 1, len(frame)):\n",
    "        X_list.append(values[t - window + 1 : t + 1])  # shape (window,)\n",
    "        y_list.append(targets[t])                       # scalar\n",
    "        idx_list.append(frame[\"date\"].iloc[t])          # for alignment/plotting\n",
    "    X = np.array(X_list)[:, :, None]                    # (samples, window, features=1)\n",
    "    y = np.array(y_list)\n",
    "    idx = pd.to_datetime(idx_list)\n",
    "    return X, y, idx\n",
    "\n",
    "X_train, y_train, idx_train = make_sequences(train_df, WINDOW)\n",
    "X_test,  y_test,  idx_test  = make_sequences(test_df,  WINDOW)\n",
    "\n",
    "# Optional: Make a small validation split from the tail of the training set (time-aware)\n",
    "val_size = max( int(0.1 * len(X_train)), 1 )\n",
    "X_tr, y_tr = X_train[:-val_size], y_train[:-val_size]\n",
    "X_val, y_val = X_train[-val_size:], y_train[-val_size:]\n",
    "\n",
    "print(f\"Train sequences: {len(X_tr):,}  |  Val sequences: {len(X_val):,}  |  Test sequences: {len(X_test):,}\")\n",
    "print(\"Input shape:\", X_tr.shape, \" Target shape:\", y_tr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 6) DEFINE LSTM MODEL\n",
    "# -----------------------------\n",
    "def build_model(window):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(window, 1)),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1)  # predicting log(target_5d)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=LR),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(WINDOW)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798800cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 7) TRAIN\n",
    "# -----------------------------\n",
    "es = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_plateau = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=max(PATIENCE // 2, 2),\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks=[es, lr_plateau]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 8) EVALUATION\n",
    "#     a) Log scale (y_log)\n",
    "#     b) Original scale (exp)\n",
    "# -----------------------------\n",
    "# Predictions\n",
    "y_pred_test_log = model.predict(X_test).ravel()\n",
    "y_pred_test = np.exp(y_pred_test_log)\n",
    "y_true_test = np.exp(y_test)\n",
    "\n",
    "# Log scale metrics\n",
    "mse_log = mean_squared_error(y_test, y_pred_test_log)\n",
    "rmse_log = np.sqrt(mse_log)\n",
    "mae_log = mean_absolute_error(y_test, y_pred_test_log)\n",
    "r2_log  = r2_score(y_test, y_pred_test_log)\n",
    "\n",
    "print(\"\\nLog scale metrics:\")\n",
    "print(f\"  RMSE (log): {rmse_log:.4f}\")\n",
    "print(f\"  MAE  (log): {mae_log:.4f}\")\n",
    "print(f\"  R²   (log): {r2_log:.4f}\")\n",
    "\n",
    "# Original scale metrics\n",
    "mse = mean_squared_error(y_true_test, y_pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "mae  = mean_absolute_error(y_true_test, y_pred_test)\n",
    "mape = (np.abs(y_pred_test - y_true_test) / y_true_test).mean() * 100\n",
    "\n",
    "print(\"\\nOriginal scale metrics:\")\n",
    "print(f\"  RMSE: {rmse:,.0f}\")\n",
    "print(f\"  MAE : {mae:,.0f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 9) PLOTS\n",
    "# -----------------------------\n",
    "# Training history (loss)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE, log scale)\")\n",
    "plt.title(\"LSTM Training History\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Actual vs Predicted (log)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_test, y_pred_test_log, alpha=0.6)\n",
    "lo, hi = min(y_test.min(), y_pred_test_log.min()), max(y_test.max(), y_pred_test_log.max())\n",
    "plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
    "plt.xlabel(\"Actual log(target_5d)\")\n",
    "plt.ylabel(\"Predicted log(target_5d)\")\n",
    "plt.title(\"LSTM: Actual vs Predicted (log scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series (original scale)\n",
    "ts_plot = pd.DataFrame({\n",
    "    \"date\": idx_test,\n",
    "    \"actual_5d_volume\": y_true_test,\n",
    "    \"predicted_5d_volume\": y_pred_test\n",
    "}).sort_values(\"date\")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ts_plot[\"date\"], ts_plot[\"actual_5d_volume\"], label=\"Actual\", alpha=0.9)\n",
    "plt.plot(ts_plot[\"date\"], ts_plot[\"predicted_5d_volume\"], label=\"Predicted\", alpha=0.9)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"5-day accumulated trading volume\")\n",
    "plt.title(\"LSTM: Actual vs Predicted (original scale)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 10) QUICK SANITY CHECK: window alignment\n",
    "# -----------------------------\n",
    "# For a random test index, ensure the last element of the sequence is exactly today's log_vol,\n",
    "# and the target corresponds to the precomputed target_5d at the same index.\n",
    "rand_idx = np.random.randint(0, len(X_test))\n",
    "print(\"\\nSanity check at one test sample:\")\n",
    "print(\"Sequence window ends at date:\", idx_test.iloc[rand_idx])\n",
    "print(\"Sequence shape:\", X_test[rand_idx].shape, \"— last timestep feature value:\", X_test[rand_idx][-1, 0])\n",
    "print(\"Pred log target:\", y_pred_test_log[rand_idx], \" | True log target:\", y_test[rand_idx])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
